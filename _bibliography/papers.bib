---
---

@string{aps = {American Physical Society,}}

@article{yan2025larger,
  title={Larger Datasets Can Be Repeated More: A Theoretical Analysis of Multi-Epoch Scaling in Linear Regression},
  author={Yan*, Tingkai and Wen*, Haodong and Li*, Binghui and Luo, Kairong and Chen, Wenguang and Lyu, Kaifeng},
  year={2026},
  abbr={ICLR},
  arxiv={2511.13421},
  additional_info = {(*: equal contribution)}
}

@article{luo2025learning,
  title={How Learning Rate Decay Wastes Your Best Data in Curriculum-Based LLM Pretraining},
  author={Luo, Kairong and Sun, Zhenbo and Wen, Haodong and Shi, Xinyu and Cui, Jiarui and Dang, Chenyi and Lyu, Kaifeng and Chen, Wenguang},
  year={2026},
  abbr={ICLR},
  arxiv={2511.18903},
}

@article{li2025adam,
  title={Adam Reduces a Unique Form of Sharpness: Theoretical Insights Near the Minimizer Manifold},
  author={Li&, Xinghan and Wen&, Haodong and Lyu, Kaifeng},
  year={2025},
  abbr={NeurIPS},
  arxiv={2511.02773},
  additional_info = {(&: equal contribution; alphabet ordering)}
}

@article{luo2025multi,
  title={A multi-power law for loss curve prediction across learning rate schedules},
  author={Luo, Kairong and Wen, Haodong and Hu, Shengding and Sun, Zhenbo and Liu, Zhiyuan and Sun, Maosong and Lyu, Kaifeng and Chen, Wenguang},
  year={2025},
  abbr={ICLR},
  arxiv={2503.12811}
}

@article{cheng2025exploring,
  title={Exploring the robustness of in-context learning with noisy labels},
  author={Cheng*, Chen and Yu*, Xinzhi and Wen*, Haodong and Sun, Jingsong and Yue, Guanzhang and Zhang, Yihao and Wei, Zeming},
  year={2025},
  abbr={ICASSP},
  arxiv={2404.18191},
  additional_info = {(*: equal contribution)}
}
